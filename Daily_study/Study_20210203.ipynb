{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "velvet-emerald",
   "metadata": {},
   "source": [
    "# **학업 일지**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separate-element",
   "metadata": {},
   "source": [
    "#### 오늘의 한마디  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-today",
   "metadata": {},
   "source": [
    "> **Convolution을 활용하여 이미지를 트레이닝시키는 것 적용하기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-chocolate",
   "metadata": {},
   "source": [
    "## 1. 강의 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-productivity",
   "metadata": {},
   "source": [
    "### 1.1 Convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-petersburg",
   "metadata": {},
   "source": [
    "#### 1.1.1 convolution의 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-innocent",
   "metadata": {},
   "source": [
    "- Continuous convolution  \n",
    "![image.png](img/con_convolution.png)  \n",
    "\n",
    "신호처리에서 실제 원하는 신호와 얼마나 연관성이 있는지를 나타내는 지표가 될 수 있다.  \n",
    "\n",
    "- Discrete convolution  \n",
    "![image.png](img/dis_convolution.png) \n",
    "\n",
    "좁은 영역에 대해서 kernal과 그림사이의 연관성을 분석하여 특징을 찾아내는 역할을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-behavior",
   "metadata": {},
   "source": [
    "### 1.2 Modern CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-gabriel",
   "metadata": {},
   "source": [
    "#### 1.2.1 우승작들의 역사와 개선점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nervous-vision",
   "metadata": {},
   "source": [
    "- AlexNet ( 8-layer, 60 M 파라미터 )  \n",
    "GPU사용 ,ReLU, Dropout, Data augmentation  \n",
    "11, 11 convolution mask\n",
    "- VGGNet  ( 19-layer, 110M 파라미터 )  \n",
    "3,3 convolution mask : layer를 늘릴수있다. \n",
    "- GoogLeNet ( 22-layer, 4 M 파라미터 )  \n",
    "Network in Network 구조, 1,1 convolution(channel방향 demension 줄이기), inception block,\n",
    "- ResNet  \n",
    "residual connection(1,1 convolution) 추가한다. 일반적으로 simple shortcut을 사용한다. Batch Norm\n",
    "batchnorm과 relu의 순서는 무엇이 나을까?\n",
    "Bottlenect architecture 3,3 convolution 앞뒤에 1,1 convolution을 추가시킨다.- 채널맞추기\n",
    "![image.png](img/resnet.png) \n",
    "\n",
    "- DenseNet  \n",
    "resnet은 더하는 것이지만 DenseNet은 concatenate시켜준다. 커지는 channel은 1,1 convolution을 활용한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-adelaide",
   "metadata": {},
   "source": [
    "### 1.3 Semantic Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-dining",
   "metadata": {},
   "source": [
    "#### Semantic Segmentation의 역사와 개선점"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-spain",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "spanish-illness",
   "metadata": {},
   "source": [
    "### 1.4 CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-apartment",
   "metadata": {},
   "source": [
    "#### 1.2.1 Cnn model code review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chemical-species",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version:[1.7.1].\n",
      "device:[cuda:0].\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "print (\"PyTorch version:[%s].\"%(torch.__version__))\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print (\"device:[%s].\"%(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mysterious-channels",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_train:\n",
      " Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: ToTensor() \n",
      "\n",
      "mnist_test:\n",
      " Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./data/\n",
      "    Split: Test\n",
      "    StandardTransform\n",
      "Transform: ToTensor() \n",
      "\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# dataset 가져와서 batch size로 나누어 iterater 만들기\n",
    "from torchvision import datasets,transforms\n",
    "mnist_train = datasets.MNIST(root='./data/',train=True,transform=transforms.ToTensor(),download=True)\n",
    "mnist_test = datasets.MNIST(root='./data/',train=False,transform=transforms.ToTensor(),download=True)\n",
    "print (\"mnist_train:\\n\",mnist_train,\"\\n\")\n",
    "print (\"mnist_test:\\n\",mnist_test,\"\\n\")\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "piano-found",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "train_iter = torch.utils.data.DataLoader(mnist_train,batch_size=BATCH_SIZE,shuffle=True,num_workers=1)\n",
    "test_iter = torch.utils.data.DataLoader(mnist_test,batch_size=BATCH_SIZE,shuffle=True,num_workers=1)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceramic-creation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "class ConvolutionalNeuralNetworkClass(nn.Module):\n",
    "    \"\"\"\n",
    "        Convolutional Neural Network (CNN) Class\n",
    "    \"\"\"\n",
    "    # OOP 개념을 응용하여 모델 정의하기\n",
    "    def __init__(self,name='cnn',xdim=[1,28,28],\n",
    "                 ksize=3,cdims=[32,64],hdims=[1024,128],ydim=10,\n",
    "                 USE_BATCHNORM=False):\n",
    "        super(ConvolutionalNeuralNetworkClass,self).__init__()\n",
    "        self.name = name\n",
    "        self.xdim = xdim\n",
    "        self.ksize = ksize\n",
    "        self.cdims = cdims\n",
    "        self.hdims = hdims\n",
    "        self.ydim = ydim\n",
    "        self.USE_BATCHNORM = USE_BATCHNORM\n",
    "\n",
    "        # Convolutional layers\n",
    "        self.layers = []\n",
    "        prev_cdim = self.xdim[0]\n",
    "        for cdim in self.cdims: # for each hidden layer\n",
    "            self.layers.append(\n",
    "                nn.Conv2d(prev_cdim,cdim,kernel_size=self.ksize,stride=(1,1),\n",
    "                          padding=(self.ksize//2)\n",
    "                  )) # convlution\n",
    "            if self.USE_BATCHNORM :\n",
    "                self.layers.append(nn.BatchNorm2d(cdim)) # batch-norm\n",
    "            self.layers.append(nn.ReLU(True))  # activation\n",
    "            self.layers.append(nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))) # max-pooling \n",
    "            self.layers.append(nn.Dropout2d(p=0.5))  # dropout\n",
    "            prev_cdim = cdim\n",
    "\n",
    "        # Dense layers\n",
    "        self.layers.append(nn.Flatten())\n",
    "        prev_hdim = prev_cdim*(self.xdim[1]//(2**len(self.cdims)))*(self.xdim[2]//(2**len(self.cdims)))\n",
    "        for hdim in self.hdims:\n",
    "            self.layers.append(nn.Linear(prev_hdim,hdim\n",
    "                # FILL IN HERE\n",
    "                               ))\n",
    "            self.layers.append(nn.ReLU(True))  # activation\n",
    "            prev_hdim = hdim\n",
    "        # Final layer (without activation)\n",
    "        self.layers.append(nn.Linear(prev_hdim,self.ydim,bias=True))\n",
    "\n",
    "        # Concatenate all layers \n",
    "        self.net = nn.Sequential()\n",
    "        for l_idx,layer in enumerate(self.layers):\n",
    "            layer_name = \"%s_%02d\"%(type(layer).__name__.lower(),l_idx)\n",
    "            self.net.add_module(layer_name,layer)\n",
    "        self.init_param() # initialize parameters\n",
    "        \n",
    "    def init_param(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m,nn.Conv2d): # init conv\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m,nn.BatchNorm2d): # init BN\n",
    "                nn.init.constant_(m.weight,1)\n",
    "                nn.init.constant_(m.bias,0)\n",
    "            elif isinstance(m,nn.Linear): # lnit dense\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            \n",
    "    def forward(self,x):\n",
    "        return self.net(x)\n",
    "\n",
    "C = ConvolutionalNeuralNetworkClass(\n",
    "    name='cnn',xdim=[1,28,28],ksize=3,cdims=[32,64],\n",
    "    hdims=[32],ydim=10).to(device)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optm = optim.Adam(C.parameters(),lr=1e-3)\n",
    "print (\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arbitrary-wallet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] name:[net.conv2d_00.weight] shape:[(32, 1, 3, 3)].\n",
      "    val:[-0.192 -0.124  0.626 -0.057  0.378]\n",
      "[1] name:[net.conv2d_00.bias] shape:[(32,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[2] name:[net.conv2d_04.weight] shape:[(64, 32, 3, 3)].\n",
      "    val:[ 0.091  0.074  0.144 -0.056  0.062]\n",
      "[3] name:[net.conv2d_04.bias] shape:[(64,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[4] name:[net.linear_09.weight] shape:[(32, 3136)].\n",
      "    val:[-0.018  0.015 -0.033  0.058 -0.064]\n",
      "[5] name:[net.linear_09.bias] shape:[(32,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "[6] name:[net.linear_11.weight] shape:[(10, 32)].\n",
      "    val:[ 0.36  -0.283  0.122  0.275 -0.303]\n",
      "[7] name:[net.linear_11.bias] shape:[(10,)].\n",
      "    val:[0. 0. 0. 0. 0.]\n",
      "Total number of parameters:[119,530].\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "n_param = 0\n",
    "for p_idx,(param_name,param) in enumerate(C.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        param_numpy = param.detach().cpu().numpy() # to numpy array \n",
    "        n_param += len(param_numpy.reshape(-1))\n",
    "        print (\"[%d] name:[%s] shape:[%s].\"%(p_idx,param_name,param_numpy.shape))\n",
    "        print (\"    val:%s\"%(param_numpy.reshape(-1)[:5]))\n",
    "print (\"Total number of parameters:[%s].\"%(format(n_param,',d')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "preliminary-precipitation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_torch:\n",
      " tensor([[[[0.383, 0.025, 0.250,  ..., 0.134, 0.644, 0.542],\n",
      "          [0.115, 0.283, 0.608,  ..., 0.169, 0.246, 0.836],\n",
      "          [0.007, 0.488, 0.788,  ..., 0.784, 0.744, 0.688],\n",
      "          ...,\n",
      "          [0.440, 0.420, 0.908,  ..., 0.231, 0.566, 0.682],\n",
      "          [0.436, 0.177, 0.798,  ..., 0.251, 0.807, 0.990],\n",
      "          [0.373, 0.446, 0.004,  ..., 0.403, 0.380, 0.007]]],\n",
      "\n",
      "\n",
      "        [[[0.687, 0.169, 0.155,  ..., 0.124, 0.156, 0.531],\n",
      "          [0.783, 0.847, 0.354,  ..., 0.457, 0.475, 0.990],\n",
      "          [0.655, 0.147, 0.573,  ..., 0.376, 0.823, 0.817],\n",
      "          ...,\n",
      "          [0.364, 0.262, 0.130,  ..., 0.103, 0.679, 0.591],\n",
      "          [0.025, 0.484, 0.892,  ..., 0.987, 0.951, 0.148],\n",
      "          [0.581, 0.100, 0.255,  ..., 0.383, 0.184, 0.907]]]], device='cuda:0')\n",
      "y_torch:\n",
      " tensor([[-1.153, -4.308, -1.840, -2.702,  0.961,  0.573, -7.581, -1.540, -4.667,\n",
      "          0.091],\n",
      "        [ 0.765, -3.300, -1.796, -3.842, -1.426, -1.519, -2.906,  0.429, -0.654,\n",
      "          1.724]], device='cuda:0', grad_fn=<AddmmBackward>)\n",
      "\n",
      "x_numpy (2, 1, 28, 28):\n",
      " [[[[0.383 0.025 0.25  ... 0.134 0.644 0.542]\n",
      "   [0.115 0.283 0.608 ... 0.169 0.246 0.836]\n",
      "   [0.007 0.488 0.788 ... 0.784 0.744 0.688]\n",
      "   ...\n",
      "   [0.44  0.42  0.908 ... 0.231 0.566 0.682]\n",
      "   [0.436 0.177 0.798 ... 0.251 0.807 0.99 ]\n",
      "   [0.373 0.446 0.004 ... 0.403 0.38  0.007]]]\n",
      "\n",
      "\n",
      " [[[0.687 0.169 0.155 ... 0.124 0.156 0.531]\n",
      "   [0.783 0.847 0.354 ... 0.457 0.475 0.99 ]\n",
      "   [0.655 0.147 0.573 ... 0.376 0.823 0.817]\n",
      "   ...\n",
      "   [0.364 0.262 0.13  ... 0.103 0.679 0.591]\n",
      "   [0.025 0.484 0.892 ... 0.987 0.951 0.148]\n",
      "   [0.581 0.1   0.255 ... 0.383 0.184 0.907]]]]\n",
      "y_numpy (2, 10):\n",
      " [[-1.153 -4.308 -1.84  -2.702  0.961  0.573 -7.581 -1.54  -4.667  0.091]\n",
      " [ 0.765 -3.3   -1.796 -3.842 -1.426 -1.519 -2.906  0.429 -0.654  1.724]]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "torch.set_printoptions(precision=3)\n",
    "x_numpy = np.random.rand(2,1,28,28)\n",
    "x_torch = torch.from_numpy(x_numpy).float().to(device)\n",
    "y_torch = C.forward(x_torch) # forward path\n",
    "y_numpy = y_torch.detach().cpu().numpy() # torch tensor to numpy array\n",
    "print (\"x_torch:\\n\",x_torch)\n",
    "print (\"y_torch:\\n\",y_torch)\n",
    "print (\"\\nx_numpy %s:\\n\"%(x_numpy.shape,),x_numpy)\n",
    "print (\"y_numpy %s:\\n\"%(y_numpy.shape,),y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "prepared-allergy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def func_eval(model,data_iter,device):\n",
    "    with torch.no_grad():\n",
    "        n_total,n_correct = 0,0\n",
    "        model.eval() # evaluate (affects DropOut and BN)\n",
    "        for batch_in,batch_out in data_iter:\n",
    "            y_trgt = batch_out.to(device)\n",
    "            model_pred = model(batch_in.view(-1,1,28,28).to(device))\n",
    "            _,y_pred = torch.max(model_pred.data,1)\n",
    "            n_correct += (y_pred==y_trgt).sum().item()\n",
    "            n_total += batch_in.size(0)\n",
    "        val_accr = (n_correct/n_total)\n",
    "        model.train() # back to train mode \n",
    "    return val_accr\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "favorite-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accr:[0.102] test_accr:[0.104].\n"
     ]
    }
   ],
   "source": [
    "C.init_param() # initialize parameters\n",
    "train_accr = func_eval(C,train_iter,device)\n",
    "test_accr = func_eval(C,test_iter,device)\n",
    "print (\"train_accr:[%.3f] test_accr:[%.3f].\"%(train_accr,test_accr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ceramic-balance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training.\n",
      "epoch:[0] loss:[0.648] train_accr:[0.953] test_accr:[0.955].\n",
      "epoch:[1] loss:[0.195] train_accr:[0.971] test_accr:[0.969].\n",
      "epoch:[2] loss:[0.136] train_accr:[0.980] test_accr:[0.979].\n",
      "epoch:[3] loss:[0.107] train_accr:[0.983] test_accr:[0.983].\n",
      "epoch:[4] loss:[0.091] train_accr:[0.984] test_accr:[0.985].\n",
      "epoch:[5] loss:[0.085] train_accr:[0.987] test_accr:[0.986].\n",
      "epoch:[6] loss:[0.077] train_accr:[0.989] test_accr:[0.987].\n",
      "epoch:[7] loss:[0.069] train_accr:[0.990] test_accr:[0.988].\n",
      "epoch:[8] loss:[0.066] train_accr:[0.991] test_accr:[0.987].\n",
      "epoch:[9] loss:[0.062] train_accr:[0.992] test_accr:[0.988].\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print (\"Start training.\")\n",
    "C.init_param() # initialize parameters\n",
    "C.train() # to train mode \n",
    "EPOCHS,print_every = 10,1\n",
    "for epoch in range(EPOCHS):\n",
    "    loss_val_sum = 0\n",
    "    for batch_in,batch_out in train_iter:\n",
    "        # Forward path\n",
    "        y_pred = C.forward(batch_in.view(-1,1,28,28).to(device))\n",
    "        loss_out = loss(y_pred,batch_out.to(device))\n",
    "        # Update\n",
    "        optm.zero_grad()     # reset gradient \n",
    "\n",
    "        loss_out.backward()    # backpropagate\n",
    "        optm.step()      # optimizer update\n",
    "        loss_val_sum += loss_out\n",
    "    loss_val_avg = loss_val_sum/len(train_iter)\n",
    "    # Print\n",
    "    if ((epoch%print_every)==0) or (epoch==(EPOCHS-1)):\n",
    "        train_accr = func_eval(C,train_iter,device)\n",
    "        test_accr = func_eval(C,test_iter,device)\n",
    "        print (\"epoch:[%d] loss:[%.3f] train_accr:[%.3f] test_accr:[%.3f].\"%\n",
    "               (epoch,loss_val_avg,train_accr,test_accr))\n",
    "print (\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-reliance",
   "metadata": {},
   "source": [
    "## 2. 피어 세션 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defined-bullet",
   "metadata": {},
   "source": [
    "#### 2.1 스터디"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-sport",
   "metadata": {},
   "source": [
    "- 2.1.1 bootstrapping에 대해 개념을 잡았고 k-fold validation에 대해 토론을 하였다.(발표: 변재경)\n",
    "\n",
    "- 2.1.2 CNN에 들어가는 개념을 나누어 분석하였고 개념이 통일되도록 정리를 하였다.(발표: 김상현)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-knight",
   "metadata": {},
   "source": [
    "#### 2.2 원하는 진로를 잡고 논문 공부"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bronze-ambassador",
   "metadata": {},
   "source": [
    "- Deep Learning : Yann LeCnn, Yoshua Bengio & Geoffrey Hinton\n",
    "딥러닝에 대한 전반적인 이해를 돕고 AI 논문에 대한 언어 장벽을 없애기 위해 필요하며 발표 예정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-economy",
   "metadata": {},
   "source": [
    "## 3. 진행중인 공부 및 신규 공부 목록"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-switch",
   "metadata": {},
   "source": [
    "- 진행중인 공부  \n",
    "\n",
    "    - Deep learning 논문 읽고 정리하기\n",
    "    - AI 기본 수학 : Mathematics for Machine learning - Marc Peter Deisenroth 3과 공부\n",
    "    - 웹 크롤링 및 데이터 처리 연습 익숙해지기\n",
    "    - Numpy를 이용하여 프로젝트 하나 진행해보기 (wav파일로 악보를 추출하는 것을 계획중)\n",
    "    - Pandas 연산 반복 학습 하기\n",
    "    - Pytorch로 시작하는 딥러닝, Pytorch tutorial 공부  \n",
    "    - pytorch 데이터셋 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-attachment",
   "metadata": {},
   "source": [
    "- 신규 공부 목록  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-motor",
   "metadata": {},
   "source": [
    "- 완료한 공부  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-effects",
   "metadata": {},
   "source": [
    "## 4. 감사한 일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-appliance",
   "metadata": {},
   "source": [
    "- dataset에 대해 모르는 것을 줌으로 도와줄 수 있느냐는 질문에 흔쾌히 가르쳐준 새봄님에게 감사합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
